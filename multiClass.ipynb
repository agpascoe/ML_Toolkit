{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'ML (Python 3.7.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'ML (Python 3.7.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from jagpascoe_ML_toolkit.createDataFunctions import createQwerties, splitData\n",
    "data, labels = createQwerties(clusters=5, nPerClust=80, blur=1.20, centroids=np.array([[0,0],[0,5],[0,-5],[5,0],[5,-5]]), draw=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader, dev_loader, test_loader = splitData(partitions=[0.85,0.075], batch_size=30, data=data, labels=labels, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class mClass(nn.Module):\n",
    "    '''\n",
    "    this Class models a Multi classificator in Pytorch\n",
    "    '''\n",
    "  \n",
    "\n",
    "    def __init__(self, nImputs, nOutputs, hidden, nHidden, activation=F.relu, dr=0.0):\n",
    "      super().__init__()\n",
    "\n",
    "      #Create a dictionary to save the layers\n",
    "      self.layers = nn.ModuleDict()\n",
    "      self.nImputs = nImputs\n",
    "      self.nOutputs = nOutputs\n",
    "      self.hidden = hidden\n",
    "      self.nHidden = nHidden\n",
    "      self.dr = dr #Dropuot rate\n",
    "      self.activation=activation\n",
    "\n",
    "      self.trainAcc = [] #for storing the accuracy vectors of trainning and dev\n",
    "      self.devAcc = []\n",
    "      self.losses = [] #for storing losses at trainning\n",
    "\n",
    "      #This two vectors are for measure weight changes and withconds of the learnig process\n",
    "      self.weightChanges = [] #This counts about how much is learning in each epoch\n",
    "      self.weightConds = [] #this how much is specializing in anykind of particularlity\n",
    "      \n",
    "      ### input layer\n",
    "      self.layers[\"input\"] = nn.Linear(nImputs,nHidden)\n",
    "\n",
    "      for h in range(self.hidden):      \n",
    "      ### hidden layer\n",
    "        self.layers[f'hidden{h}'] = nn.Linear(nHidden,nHidden)\n",
    "\n",
    "      ### output layer\n",
    "      self.layers['output'] = nn.Linear(nHidden,nOutputs)\n",
    "\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self,x):\n",
    "      x = self.activation( self.layers['input'](x) )\n",
    "      x = F.dropout(x,p=self.dr,training=self.training) # switch dropout off during .eval()\n",
    "   \n",
    "      for h in range(self.hidden):\n",
    "        x = self.activation( self.layers[f'hidden{h}'](x) )\n",
    "        x = F.dropout(x,p=self.dr,training=self.training) # switch dropout off during .eval()\n",
    "        #x = F.softmax(x)\n",
    "      return self.layers['output'](x)\n",
    "   \n",
    "   \n",
    "   \n",
    "    def testModel(self, n, test_loader):\n",
    "      # extract X,y from test dataloader\n",
    "      X,y = next(iter(test_loader)) \n",
    "      self.load_state_dict(n['net'])\n",
    "      yHat = self.forward(X)\n",
    "      bestAcc = 100*torch.mean((torch.argmax(yHat,axis=1)==y).float())\n",
    "      return(bestAcc)\n",
    "\n",
    "\n",
    "    def fit(self, train_loader, dev_loader, numepochs = 10, learningRate=0.01, lossFunction= nn.CrossEntropyLoss, \n",
    "            optimizer=torch.optim.Adam, weight_decay=0, initializer=None):\n",
    "            \n",
    "      # Initialize a dictionary for the best model\n",
    "      theBestModel = {'Accuracy':0, 'net':None, 'epoch':0} #net will be the whole model instance\n",
    "\n",
    "      self.weightChanges = np.zeros((numepochs, self.hidden+2)) #This two vectors are for measure weight changes and withconds of the learnig process\n",
    "      self.weightConds = np.zeros((numepochs, self.hidden+2))\n",
    "\n",
    "      if initializer == 'xavier':         #this set the methgod for iniatiazing weights\n",
    "        for p in self.named_parameters():\n",
    "          if 'weight' in p[0]:\n",
    "            nn.init.xavier_normal_(p[1].data)\n",
    "      elif initializer == 'kaiming': #it's kaiming\n",
    "        for p in self.named_parameters():\n",
    "          if 'weight' in p[0]:\n",
    "            nn.init.kaiming_uniform_(p[1].data, nonlinearity='relu') #be sure that relu or leaky_relu is the activation\n",
    "\n",
    "      # loss function\n",
    "      self.lossfun = lossFunction()\n",
    "\n",
    "      # optimizer\n",
    "      self.optimizer = optimizer(params=self.parameters(), lr=learningRate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "      # initialize losses\n",
    "      self.losses   = torch.zeros(numepochs)\n",
    "      self.trainAcc = []\n",
    "      self.devAcc   = []\n",
    "\n",
    "      # loop over epochs\n",
    "      for epochi in range(numepochs):\n",
    "\n",
    "        # store the weights for each layer\n",
    "        preW = []\n",
    "        for p in self.named_parameters():\n",
    "          if 'weight' in p[0]:\n",
    "            preW.append( copy.deepcopy(p[1].data.numpy()) )\n",
    "        \n",
    "        \n",
    "        # switch on training mode\n",
    "        self.train()\n",
    "\n",
    "        # loop over training data batches\n",
    "        batchAcc  = []\n",
    "        batchLoss = []\n",
    "        for X,y in train_loader:\n",
    "\n",
    "          # forward pass and loss\n",
    "          yHat = self.forward(X)\n",
    "          loss = self.lossfun(yHat,y)\n",
    "\n",
    "          # backprop\n",
    "          self.optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          self.optimizer.step()\n",
    "\n",
    "          # loss from this batch\n",
    "          batchLoss.append(loss.item())\n",
    "\n",
    "          # compute accuracy\n",
    "          matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
    "          matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
    "          accuracyPct = 100*torch.mean(matchesNumeric) # average and x100 \n",
    "          batchAcc.append( accuracyPct )               # add to list of accuracies\n",
    "          # end of batch loop...\n",
    "\n",
    "        # now that we've trained through the batches, get their average training accuracy\n",
    "        self.trainAcc.append( np.mean(batchAcc) )\n",
    "\n",
    "        # and get average losses across the batches\n",
    "        self.losses[epochi] = np.mean(batchLoss)\n",
    "\n",
    "        # test accuracy\n",
    "        self.eval()\n",
    "        X,y = next(iter(dev_loader)) # extract X,y from devset dataloader\n",
    "        with torch.no_grad(): # deactivates autograd\n",
    "          yHat = self.forward(X)\n",
    "      \n",
    "        # compare the following really long line of code to the training accuracy lines\n",
    "        self.devAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "\n",
    "        # New! Store this model if it's the best so far\n",
    "        if self.devAcc[-1]>theBestModel['Accuracy']:\n",
    "      \n",
    "          # new best accuracy\n",
    "          theBestModel['Accuracy'] = self.devAcc[-1].item()\n",
    "      \n",
    "          # epoch iteration\n",
    "          theBestModel['epoch'] = epochi\n",
    "\n",
    "          # model's internal state\n",
    "          theBestModel['net'] = copy.deepcopy( self.state_dict() ) #here update the best model\n",
    "      \n",
    "        # Get the post-learning state of the weights\n",
    "        for (i,p) in enumerate(self.named_parameters()):\n",
    "          if 'weight' in p[0]:\n",
    "          # condition number, just for weights not bias\n",
    "            self.weightConds[epochi,int(i/2)]  = np.linalg.cond(p[1].data)\n",
    "\n",
    "            # Frobenius norm of the weight change from pre-learning\n",
    "            self.weightChanges[epochi,int(i/2)] = np.linalg.norm( preW[int(i/2)]-p[1].data.numpy(), ord='fro')\n",
    "\n",
    "  \n",
    "        # end epochs\n",
    "\n",
    "      # function output\n",
    "      return self.trainAcc,self.devAcc,self.losses,theBestModel\n",
    "\n",
    "    def drawModel(self, figsize=(15,18)):\n",
    "      #plot results of trainning\n",
    "      # set up the plot\n",
    "      fig,ax = plt.subplots(3,2,figsize=figsize)\n",
    "      ax = ax.flatten()\n",
    "\n",
    "      # losses\n",
    "      ax[0].plot(self.losses.detach(),'-')\n",
    "      ax[0].set_ylabel('Loss')\n",
    "      ax[0].set_xlabel('epoch')\n",
    "      ax[0].set_title('Losses')\n",
    "      #ax[0].set_ylim([0,0.6])\n",
    "\n",
    "      #accuracy\n",
    "      ax[1].plot(self.trainAcc,'-',label='Train')\n",
    "      ax[1].plot(self.devAcc,'-',label='Devset')\n",
    "      ax[1].set_ylabel('devAccuracy (%)')\n",
    "      ax[1].set_xlabel('Epoch')\n",
    "      ax[1].set_title('Accuracy')\n",
    "      #ax[1].set_ylim([85,95])\n",
    "      #ax[1].set_xlim([80,105])\n",
    "      ax[1].legend()\n",
    "      \n",
    "      layername = []\n",
    "      for (i,p) in enumerate(self.named_parameters()):\n",
    "        if 'weight' in p[0]:\n",
    "          layername.append(p[0][:-7])\n",
    "      \n",
    "      # weight changes\n",
    "      # get a list of layer names\n",
    "      \n",
    "      ax[2].plot(self.weightChanges)\n",
    "      ax[2].set_xlabel('Epochs')\n",
    "      ax[2].set_title('Weight change from previous epoch')\n",
    "      ax[2].legend(layername)\n",
    "      #ax[2].set_ylim([0,0.2])\n",
    "      #ax[2].set_yscale('log')\n",
    "\n",
    "      # weight condition numbers\n",
    "      ax[3].plot(self.weightConds)\n",
    "      ax[3].set_xlabel('Epochs')\n",
    "      ax[3].set_title('Condition number')\n",
    "      ax[3].legend(layername)\n",
    "      #ax[3].set_ylim([0,1000])\n",
    "      #ax[3].set_yscale('log')\n",
    "\n",
    "     #Graph an histogram of weights\n",
    "     # store the weights for each layer\n",
    "      preW = []\n",
    "      for p in self.named_parameters():\n",
    "        if 'weight' in p[0]:\n",
    "           preW = np.append(preW, p[1].data.numpy())\n",
    "\n",
    "      y,x = np.histogram(preW,30)\n",
    "      ax[4].set_title('Total weight distribution after trainning')\n",
    "      ax[4].plot((x[1:]+x[:-1])/2,y,'b')\n",
    "      ax[4].set_xlabel('Weight value')\n",
    "      ax[4].set_ylabel('Count')\n",
    "\n",
    "    \n",
    "      from scipy.stats import zscore # normalize for scaling offsets\n",
    "      #using zcore is only for vision convinience\n",
    "\n",
    "      ax[5].plot(zscore(np.diff(self.trainAcc)),label='d(trainAcc)')\n",
    "      ax[5].plot(zscore(np.mean(self.weightChanges,axis=1)),label='Weight change')\n",
    "      ax[5].legend()\n",
    "      ax[5].set_title('Change in weights by change in accuracy')\n",
    "      ax[5].set_xlabel('Epoch')\n",
    "      ax[5].set_ylim([-3,3])\n",
    "    \n",
    "\n",
    "      plt.tight_layout()\n",
    "      plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters=5\n",
    "n = mClass(nImputs=2, \n",
    "        nOutputs=clusters, \n",
    "        hidden = 1, \n",
    "        nHidden = 8,\n",
    "        activation=F.relu,\n",
    "        )\n",
    "\n",
    "trainAcc,devAcc,losses,theBestModel = n.fit(train_loader=train_loader, \n",
    "        dev_loader=dev_loader, \n",
    "        lossFunction=nn.CrossEntropyLoss,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        weight_decay = 0.01,\n",
    "        numepochs=20, \n",
    "        initializer='kaiming', \n",
    "        learningRate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy Devset:', theBestModel['Accuracy'])\n",
    "bestAcc = n.testModel(theBestModel, test_loader)\n",
    "print('Accuracy Test:', float(bestAcc))\n",
    "print('On iteration: ', theBestModel['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.drawModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Now, let's do it in Tensorflow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fb33fb6a31495ec95ce8fdac08b3ee5036de4a628efd8e581f718773b847e38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
